{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nHello there! In this project, we will try to predict a very imbalanced dataset - Credit Cards Frauds. As it is said in the description of the dataset, the features are transformed by a PCA (Principal Component Analysis, a Dimensionality Reduction technique) and have their names hidden for privacy reasons. We will try to explore some data and apply different classification models to come to the best solution. Let's start!","metadata":{}},{"cell_type":"markdown","source":"### Acknowledges\nSpecial regards to the authors of these notebooks, which helped me a lot to write this script!\n\n- https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets ","metadata":{}},{"cell_type":"markdown","source":"### Dependencies","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:40:21.00238Z","iopub.execute_input":"2022-01-08T15:40:21.00276Z","iopub.status.idle":"2022-01-08T15:40:22.113678Z","shell.execute_reply.started":"2022-01-08T15:40:21.002663Z","shell.execute_reply":"2022-01-08T15:40:22.112689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/creditcardfraud/creditcard.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:26.581307Z","iopub.execute_input":"2022-01-08T15:41:26.5816Z","iopub.status.idle":"2022-01-08T15:41:31.075059Z","shell.execute_reply.started":"2022-01-08T15:41:26.581569Z","shell.execute_reply":"2022-01-08T15:41:31.074196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:51.278226Z","iopub.execute_input":"2022-01-08T15:41:51.278493Z","iopub.status.idle":"2022-01-08T15:41:51.284671Z","shell.execute_reply.started":"2022-01-08T15:41:51.278465Z","shell.execute_reply":"2022-01-08T15:41:51.284066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given that the dataset is extremely imbalanced, we will use StatifiedShuffleSplit to split the train and test sets with similar proportions of our target class.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data, data['Class']):\n    df_train = data.loc[train_index]\n    df_test = data.loc[test_index]\n\nprint(df_train.shape)\nprint(df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:52.300586Z","iopub.execute_input":"2022-01-08T15:41:52.301668Z","iopub.status.idle":"2022-01-08T15:41:52.801383Z","shell.execute_reply.started":"2022-01-08T15:41:52.301606Z","shell.execute_reply":"2022-01-08T15:41:52.800286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train Set Class column:\")\nprint(df_train.Class.value_counts(normalize=True))\n\nprint(\"\\nTest Set Class column:\")\nprint(df_test.Class.value_counts(normalize=True))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:52.803326Z","iopub.execute_input":"2022-01-08T15:41:52.803767Z","iopub.status.idle":"2022-01-08T15:41:52.817138Z","shell.execute_reply.started":"2022-01-08T15:41:52.803734Z","shell.execute_reply":"2022-01-08T15:41:52.816434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have split our dataset, let's forget about Test Set and focus on our Training Set. Let's explore its data!","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:55.851134Z","iopub.execute_input":"2022-01-08T15:41:55.851631Z","iopub.status.idle":"2022-01-08T15:41:55.886181Z","shell.execute_reply.started":"2022-01-08T15:41:55.851579Z","shell.execute_reply":"2022-01-08T15:41:55.885452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns[df_train.isnull().any()]","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:56.533642Z","iopub.execute_input":"2022-01-08T15:41:56.534231Z","iopub.status.idle":"2022-01-08T15:41:56.549092Z","shell.execute_reply.started":"2022-01-08T15:41:56.534193Z","shell.execute_reply":"2022-01-08T15:41:56.548451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, we don't have any columns with null values, and also every one of them is numerical. As I said earlier, the columns were scaled and transformed by a PCA. But I was lying... not all of them! The columns `Time` and `Amount` aren't scaled. Let's explore them to scale in the best possible way.","metadata":{}},{"cell_type":"code","source":"df_train[[\"Time\", \"Amount\", \"Class\"]].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:57.520571Z","iopub.execute_input":"2022-01-08T15:41:57.521163Z","iopub.status.idle":"2022-01-08T15:41:57.579735Z","shell.execute_reply.started":"2022-01-08T15:41:57.521088Z","shell.execute_reply":"2022-01-08T15:41:57.578793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.Time.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:57.922397Z","iopub.execute_input":"2022-01-08T15:41:57.923293Z","iopub.status.idle":"2022-01-08T15:41:57.941342Z","shell.execute_reply.started":"2022-01-08T15:41:57.923255Z","shell.execute_reply":"2022-01-08T15:41:57.94053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting some parameters to plot better graphs\ncustom_params = {\"axes.spines.right\": False, \n                 \"axes.spines.top\": False,  \n                 \"font.family\": \"arial\", \n                 \"figure.figsize\": (18, 6)}\nsns.set_theme(style=\"ticks\", rc=custom_params)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:58.404002Z","iopub.execute_input":"2022-01-08T15:41:58.404708Z","iopub.status.idle":"2022-01-08T15:41:58.410924Z","shell.execute_reply.started":"2022-01-08T15:41:58.404662Z","shell.execute_reply":"2022-01-08T15:41:58.41029Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.histplot(df_train.Time, bins=100, label=\"Time\", )\ng.set_xlabel(\"Time\")\ng.set_ylabel(\"Frequency\")\ng.set_title(\"Time Distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:58.916072Z","iopub.execute_input":"2022-01-08T15:41:58.916518Z","iopub.status.idle":"2022-01-08T15:41:59.560003Z","shell.execute_reply.started":"2022-01-08T15:41:58.916475Z","shell.execute_reply":"2022-01-08T15:41:59.559065Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.boxplot(x=\"Class\", y=\"Time\", data=df_train)\ng.set_title(\"Time Distribution by Class\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:41:59.805747Z","iopub.execute_input":"2022-01-08T15:41:59.80604Z","iopub.status.idle":"2022-01-08T15:42:00.055854Z","shell.execute_reply.started":"2022-01-08T15:41:59.806007Z","shell.execute_reply":"2022-01-08T15:42:00.054951Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.histplot(df_train.Amount, bins=100, label=\"Amount\")\ng.set_ylabel(\"Frequency\")\ng.set_title(\"Amount Distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:42:00.506678Z","iopub.execute_input":"2022-01-08T15:42:00.506981Z","iopub.status.idle":"2022-01-08T15:42:01.121508Z","shell.execute_reply.started":"2022-01-08T15:42:00.506947Z","shell.execute_reply":"2022-01-08T15:42:01.120625Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.boxplot(x=\"Class\", y=\"Amount\", data=df_train)\ng.set_title(\"Amount Distribution by Class\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:42:01.123446Z","iopub.execute_input":"2022-01-08T15:42:01.123699Z","iopub.status.idle":"2022-01-08T15:42:01.411497Z","shell.execute_reply.started":"2022-01-08T15:42:01.123668Z","shell.execute_reply":"2022-01-08T15:42:01.410477Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, how many outliers! We can remove them, but we could remove rows with Class==1 (which we saw earlier that they are pretty rare and important). Another way is to use a scaling technique not very sensible to outliers, A.K.A RobustScaler! Check this amazing article by Jeff Hale about different scaling, normalizing, and standardizing techniques: https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nrob_amount = RobustScaler()\nrob_time = RobustScaler()\ndf_train['scaled_amount'] = rob_amount.fit_transform(df_train['Amount'].values.reshape(-1,1))\ndf_train['scaled_time'] = rob_time.fit_transform(df_train['Time'].values.reshape(-1,1))\n\ndf_train.drop(['Time','Amount'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:42:02.584794Z","iopub.execute_input":"2022-01-08T15:42:02.585488Z","iopub.status.idle":"2022-01-08T15:42:02.687067Z","shell.execute_reply.started":"2022-01-08T15:42:02.585435Z","shell.execute_reply":"2022-01-08T15:42:02.686426Z"},"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df_train.corr()\ng = sns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values,\n            cmap='coolwarm_r',\n            \n            )\ng.set_title(\"Linear Correlation Heatmap\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:42:03.066139Z","iopub.execute_input":"2022-01-08T15:42:03.066643Z","iopub.status.idle":"2022-01-08T15:42:04.536168Z","shell.execute_reply.started":"2022-01-08T15:42:03.06659Z","shell.execute_reply":"2022-01-08T15:42:04.535207Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, `V2` and `V5` are very negatively correlated with `scaled_amount`, and `V3` with `scaled_time`.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 1, figsize=(18,10))\ng0 = sns.scatterplot(x=\"scaled_amount\", y=\"V2\", data=df_train, ax=ax[0], hue=\"Class\", alpha=0.5)\ng1 = sns.scatterplot(x=\"scaled_amount\", y=\"V5\", data=df_train, ax=ax[1], hue=\"Class\", alpha=0.5)\ng2 = sns.scatterplot(x=\"scaled_time\", y=\"V3\", data=df_train, ax=ax[2], hue=\"Class\", alpha=0.5)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:42:04.537678Z","iopub.execute_input":"2022-01-08T15:42:04.537905Z","iopub.status.idle":"2022-01-08T15:42:25.286361Z","shell.execute_reply.started":"2022-01-08T15:42:04.537877Z","shell.execute_reply":"2022-01-08T15:42:25.285593Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,6))\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=df_train, ax=ax[0])\nsns.boxplot(x=\"Class\", y=\"V5\", data=df_train, ax=ax[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:42:25.287781Z","iopub.execute_input":"2022-01-08T15:42:25.288111Z","iopub.status.idle":"2022-01-08T15:42:25.696324Z","shell.execute_reply.started":"2022-01-08T15:42:25.288075Z","shell.execute_reply":"2022-01-08T15:42:25.695212Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training ML Models","metadata":{}},{"cell_type":"code","source":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df_train, df_train['Class']):\n    df_train_ml = data.loc[train_index]\n    df_valid = data.loc[test_index]\n\nX_train = df_train_ml.drop(['Class'], axis=1)\nX_valid = df_valid.drop(['Class'], axis=1)\ny_train = df_train_ml['Class']\ny_valid = df_valid['Class']","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:42:25.697467Z","iopub.execute_input":"2022-01-08T15:42:25.697687Z","iopub.status.idle":"2022-01-08T15:42:25.956196Z","shell.execute_reply.started":"2022-01-08T15:42:25.697661Z","shell.execute_reply":"2022-01-08T15:42:25.955211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will use RandomizedSearchCV to find the best params for our model. Given that our dataset is imbalanced, we will use a technique called Oversampling, using the SMOTE algorithm to create a dataset with synthetic positive instances.","metadata":{}},{"cell_type":"code","source":"from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.base import clone\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:42:25.958046Z","iopub.execute_input":"2022-01-08T15:42:25.958325Z","iopub.status.idle":"2022-01-08T15:42:26.420183Z","shell.execute_reply.started":"2022-01-08T15:42:25.958293Z","shell.execute_reply":"2022-01-08T15:42:26.419219Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_grid = {\n                \"n_estimators\": [100, 500, 1000],\n                \"max_depth\": [2, 5, 10],\n                \"learning_rate\": [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1.0],\n                \"gamma\": stats.reciprocal(0.001, 0.1),\n                \"subsample\": np.arange(0.1, 1.0, 0.1),\n                \"colsample_bytree\": np.arange(0.1, 1.0, 0.1),\n                \"scale_pos_weight\": [5, 10, 20, 50, 100],\n                \"n_jobs\": [-1],\n                \"use_label_encoder\": [False],\n                \"random_state\": [42]\n        }\n\nfit_params = {\n                \"early_stopping_rounds\": 5,\n                \"eval_metric\":[\"auc\"],\n                \"eval_set\": [(X_valid, y_valid)],\n                \"verbose\":0\n        }","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:48:59.333428Z","iopub.execute_input":"2022-01-08T15:48:59.334211Z","iopub.status.idle":"2022-01-08T15:48:59.343285Z","shell.execute_reply.started":"2022-01-08T15:48:59.334082Z","shell.execute_reply":"2022-01-08T15:48:59.342479Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_clf = xgb.XGBClassifier()\nrand_grid = RandomizedSearchCV(xgb_clf, \n                               params_grid, \n                               n_iter=10, \n                               cv=5, \n                               scoring=\"recall\", \n                               random_state=42,\n                               verbose=1,\n                               n_jobs=-1)\npipe = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_grid)\nrand_grid.fit(X_train, y_train, **fit_params)\npreds = rand_grid.predict(X_valid)\n\nf1 = f1_score(y_valid, preds)\nprecision = precision_score(y_valid, preds)\nrecall = recall_score(y_valid, preds)\nprint(\"F1 Score: %.3f\" %f1)\nprint(\"Precision: %.3f\" %precision)\nprint(\"Recall: %.3f\" %recall)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:49:11.775516Z","iopub.execute_input":"2022-01-08T15:49:11.775994Z","iopub.status.idle":"2022-01-08T15:54:05.39318Z","shell.execute_reply.started":"2022-01-08T15:49:11.775938Z","shell.execute_reply":"2022-01-08T15:54:05.392204Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_valid, preds, normalize='true', labels=[0,1])\nplt.figure(figsize=(6,6))\ng = sns.heatmap(cm, annot=True, fmt=\".2%\", cmap=\"Blues\")\ng.set_title(\"Validation Confusion Matrix\")\ng.set_xlabel(\"Predicted Class\")\ng.set_ylabel(\"True Class\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:54:05.710985Z","iopub.execute_input":"2022-01-08T15:54:05.711598Z","iopub.status.idle":"2022-01-08T15:54:06.022338Z","shell.execute_reply.started":"2022-01-08T15:54:05.711549Z","shell.execute_reply":"2022-01-08T15:54:06.021268Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training On Full Data","metadata":{}},{"cell_type":"code","source":"best_xgb = rand_grid.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:54:30.572243Z","iopub.execute_input":"2022-01-08T15:54:30.572519Z","iopub.status.idle":"2022-01-08T15:54:30.577134Z","shell.execute_reply.started":"2022-01-08T15:54:30.572492Z","shell.execute_reply":"2022-01-08T15:54:30.576192Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_xgb","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:54:36.424097Z","iopub.execute_input":"2022-01-08T15:54:36.424805Z","iopub.status.idle":"2022-01-08T15:54:36.437893Z","shell.execute_reply.started":"2022-01-08T15:54:36.424763Z","shell.execute_reply":"2022-01-08T15:54:36.437216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.drop(['Class'], axis=1)\ny = df_train['Class']\n\nsmote = SMOTE(sampling_strategy='minority')\nX_res, y_res = smote.fit_resample(X, y)\nbest_xgb.fit(X_res, y_res)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:55:13.373306Z","iopub.execute_input":"2022-01-08T15:55:13.373568Z","iopub.status.idle":"2022-01-08T15:58:17.65838Z","shell.execute_reply.started":"2022-01-08T15:55:13.37354Z","shell.execute_reply":"2022-01-08T15:58:17.656843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Explainability","metadata":{}},{"cell_type":"code","source":"!pip install shap","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:23:48.546315Z","iopub.execute_input":"2022-01-08T16:23:48.546953Z","iopub.status.idle":"2022-01-08T16:23:59.329457Z","shell.execute_reply.started":"2022-01-08T16:23:48.546899Z","shell.execute_reply":"2022-01-08T16:23:59.328356Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap \n\nexplainer = shap.Explainer(best_xgb)\n\nshap_values = explainer(X, check_additivity=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:24:16.440229Z","iopub.execute_input":"2022-01-08T16:24:16.440506Z","iopub.status.idle":"2022-01-08T16:31:10.598898Z","shell.execute_reply.started":"2022-01-08T16:24:16.440478Z","shell.execute_reply":"2022-01-08T16:31:10.598108Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values, X)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:31:51.843132Z","iopub.execute_input":"2022-01-08T16:31:51.843382Z","iopub.status.idle":"2022-01-08T16:32:33.072476Z","shell.execute_reply.started":"2022-01-08T16:31:51.843353Z","shell.execute_reply":"2022-01-08T16:32:33.071639Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Test Labels","metadata":{}},{"cell_type":"code","source":"df_test['scaled_amount'] = rob_amount.transform(df_test['Amount'].values.reshape(-1,1))\ndf_test['scaled_time'] = rob_time.transform(df_test['Time'].values.reshape(-1,1))\n\ndf_test.drop(['Time','Amount'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:58:17.661402Z","iopub.execute_input":"2022-01-08T15:58:17.661838Z","iopub.status.idle":"2022-01-08T15:58:17.706925Z","shell.execute_reply.started":"2022-01-08T15:58:17.661787Z","shell.execute_reply":"2022-01-08T15:58:17.705809Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = df_test.drop([\"Class\"], axis=1)\ny_test = df_test[\"Class\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:18:06.621207Z","iopub.execute_input":"2022-01-08T16:18:06.622319Z","iopub.status.idle":"2022-01-08T16:18:06.633702Z","shell.execute_reply.started":"2022-01-08T16:18:06.622258Z","shell.execute_reply":"2022-01-08T16:18:06.632744Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = best_xgb.predict(X_test)\ny_test.index = range(len(y_test))\n\ny_test_1 = y_test[y_test == 1]\ntest_preds_1 = test_preds[y_test_1.index]\n\ny_test_0 = y_test[y_test == 0]\ntest_preds_0 = test_preds[y_test_0.index]","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:19:27.450582Z","iopub.execute_input":"2022-01-08T16:19:27.450917Z","iopub.status.idle":"2022-01-08T16:19:27.458648Z","shell.execute_reply.started":"2022-01-08T16:19:27.450881Z","shell.execute_reply":"2022-01-08T16:19:27.457881Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nacc = accuracy_score(y_test, test_preds)\nacc_1 = accuracy_score(y_test_1, test_preds_1)\nacc_0 = accuracy_score(y_test_0, test_preds_0)\n\nprint(\"Total Accuracy: %.1f%%\" %(acc*100))\nprint(\"Fraud Accuracy: %.1f%%\" %(acc_1*100))\nprint(\"Non-Fraud Accuracy: %.1f%%\" %(acc_0*100))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:22:25.954456Z","iopub.execute_input":"2022-01-08T16:22:25.955142Z","iopub.status.idle":"2022-01-08T16:22:25.970476Z","shell.execute_reply.started":"2022-01-08T16:22:25.955079Z","shell.execute_reply":"2022-01-08T16:22:25.969739Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, test_preds, normalize='true', labels=[0,1])\nplt.figure(figsize=(6,6))\ng = sns.heatmap(cm, annot=True, fmt=\".2%\", cmap=\"Blues\")\ng.set_title(\"Test Confusion Matrix\")\ng.set_xlabel(\"Predicted Class\")\ng.set_ylabel(\"True Class\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:23:27.28606Z","iopub.execute_input":"2022-01-08T16:23:27.286404Z","iopub.status.idle":"2022-01-08T16:23:27.635785Z","shell.execute_reply.started":"2022-01-08T16:23:27.286368Z","shell.execute_reply":"2022-01-08T16:23:27.63514Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({\"Id\": y_test.index, \"Class\": test_preds})\noutput.to_csv(\"output.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:20:28.028086Z","iopub.execute_input":"2022-01-08T16:20:28.028521Z","iopub.status.idle":"2022-01-08T16:20:28.138892Z","shell.execute_reply.started":"2022-01-08T16:20:28.02849Z","shell.execute_reply":"2022-01-08T16:20:28.138205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you come this far, thank you! I hope I could help you in some way with my solution. Please let me know how can I improve it in the comments :)","metadata":{}}]}